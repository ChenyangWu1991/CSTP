{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the sentiment of all sentences by BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data (sentences of the full text of the FYP-Ts of 23 cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19676, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tier</th>\n",
       "      <th>Region</th>\n",
       "      <th>City</th>\n",
       "      <th>ID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Token</th>\n",
       "      <th>Car</th>\n",
       "      <th>Shared mobility</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Walking</th>\n",
       "      <th>...</th>\n",
       "      <th>Regional development balance</th>\n",
       "      <th>Alternative fuel vehicles</th>\n",
       "      <th>Electric vehicles</th>\n",
       "      <th>Autonomous vehicles</th>\n",
       "      <th>Parcel delivery</th>\n",
       "      <th>sum</th>\n",
       "      <th>Notes</th>\n",
       "      <th>SA_Li</th>\n",
       "      <th>SA_Guo</th>\n",
       "      <th>Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>East</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>“十一五”期间是北京市机动车增长最快、交通投入最大、交通结构改善最明显、交通管理最有效、市民...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>机动车</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tier Region     City  ID  Year  \\\n",
       "0    1   East  Beijing   1  2010   \n",
       "\n",
       "                                               Token  Car  Shared mobility  \\\n",
       "0  “十一五”期间是北京市机动车增长最快、交通投入最大、交通结构改善最明显、交通管理最有效、市民...  NaN              NaN   \n",
       "\n",
       "   Bike  Walking  ...  Regional development balance  \\\n",
       "0   NaN      NaN  ...                           NaN   \n",
       "\n",
       "   Alternative fuel vehicles  Electric vehicles  Autonomous vehicles  \\\n",
       "0                        NaN                NaN                  NaN   \n",
       "\n",
       "  Parcel delivery  sum  Notes  SA_Li  SA_Guo  Gap  \n",
       "0             NaN    0    机动车    NaN     NaN  NaN  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_data = pd.read_csv('all_23_cities.csv')\n",
    "print(all_data.shape)\n",
    "all_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (filter sentences by tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_token = all_data[all_data['sum'] > 0]\n",
    "all_data_same_senti_label = all_data[all_data['Gap'] == 0]\n",
    "all_data_token_same_senti_label = all_data_token[all_data_token['Gap'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6557, 29)\n",
      "(5699, 29)\n",
      "(5394, 29)\n"
     ]
    }
   ],
   "source": [
    "print(all_data_token.shape)\n",
    "print(all_data_same_senti_label.shape)\n",
    "print(all_data_token_same_senti_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_token_same_senti_label = all_data_token_same_senti_label.astype({\"SA_Li\": 'int64', \"SA_Guo\": 'int64', 'Gap': 'int64'})\n",
    "# all_data_token_same_senti_label.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 29)\n",
      "(5957, 29)\n",
      "(6557, 29)\n"
     ]
    }
   ],
   "source": [
    "train = all_data_token_same_senti_label.sample(n=600, random_state=33)\n",
    "# exclude the training set\n",
    "test = pd.concat([all_data_token, train, train]).drop_duplicates(keep=False)  # sentence level\n",
    "# test = pd.concat([all_data, train, train]).drop_duplicates(keep=False).sample(n=1000, random_state=33)  # article level\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(all_data_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结合贵阳市环城快铁、轨道交通和快速公交等交通系统的建设，促进不同交通系统专网融合以及交通专网与“天网”“公网”深度融合。\n",
      "- - - - - - - - - - - - -\n",
      "在市委、市政府的坚强领导下，交通系统深入学习实践科学发展观，振奋精神、开拓创新，克服重重困难，加快构建以“人文交通、科技交通、绿色交通”为特征的新北京交通体系，应对了5年净增200多万辆机动车的挑战，基本满足了市民不断增长和变化的交通需求，适应了首都经济社会发展的需要。\n"
     ]
    }
   ],
   "source": [
    "print(train['Token'].values[0])\n",
    "print('- - - - - - - - - - - - -')\n",
    "print(test['Token'].values[0])\n",
    "train_x = train['Token'].tolist()\n",
    "train_y = list(map(lambda x:x+1, train['SA_Li'].tolist()))\n",
    "test_x = test['Token'].tolist()\n",
    "test_y = test['SA_Li'].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use transformer to define the fine-tune model based on BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "model_name = 'hfl/chinese-bert-wwm'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# A bert fine-tuning strategy is used to adjust the parameters of the BERT and the linear layer together during back propagation to make BERT more suitable for the classification task.\n",
    "class BertClassfication(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassfication,self).__init__()\n",
    "        self.model_name = 'hfl/chinese-bert-wwm'\n",
    "        self.model = BertModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.fc = nn.Linear(768,3)     # depend on the structure of BERT, 2-layer, 768-hidden, 12-heads, 110M parameters\n",
    "        # nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self,x):               # The input is a list here.\n",
    "        batch_tokenized = self.tokenizer.batch_encode_plus(x, add_special_tokens=True,\n",
    "                                max_length=148, pad_to_max_length=True)     \n",
    "        input_ids = torch.tensor(batch_tokenized['input_ids'])\n",
    "        attention_mask = torch.tensor(batch_tokenized['attention_mask'])\n",
    "        hiden_outputs = self.model(input_ids,attention_mask=attention_mask)\n",
    "        outputs = hiden_outputs[0][:,0,:]     \n",
    "        output = self.fc(outputs)\n",
    "        return output\n",
    "model = BertClassfication()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training with batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batch_count = int(len(train) / batch_size)\n",
    "batch_train_inputs, batch_train_targets = [], []\n",
    "\n",
    "for i in range(batch_count):\n",
    "    batch_train_inputs.append(train_x[i*batch_size : (i+1)*batch_size])\n",
    "    batch_train_targets.append(train_y[i*batch_size : (i+1)*batch_size])\n",
    "\n",
    "# 初始化训练参数\n",
    "bertclassfication = BertClassfication()\n",
    "lossfuction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bertclassfication.parameters(),lr=2e-5)\n",
    "epoch = 5\n",
    "batch_count = batch_count\n",
    "print_every_batch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5, Loss: 0.8246\n",
      "Batch: 5, Loss: 0.3328\n",
      "Batch: 5, Loss: 0.2178\n",
      "Batch: 5, Loss: 0.0980\n",
      "Batch: 5, Loss: 0.0432\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epoch):\n",
    "    print_avg_loss = 0\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_train_inputs[i]\n",
    "        targets = torch.tensor(batch_train_targets[i])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bertclassfication(inputs)\n",
    "        loss = lossfuction(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print_avg_loss += loss.item()\n",
    "        if i % print_every_batch == (print_every_batch-1):\n",
    "            print(\"Batch: %d, Loss: %.4f\" % ((i+1), print_avg_loss/print_every_batch))\n",
    "            print_avg_loss = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the fine-tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# sentiment_dict = {0:'Negative', 1:'Neutral', 2:'Positive'}\n",
    "\n",
    "result = bertclassfication([train_x[0]])\n",
    "_, predict = torch.max(result,1)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results of the training set by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train = []\n",
    "\n",
    "for i in train_x:\n",
    "    result = bertclassfication([i])\n",
    "    _, predict = torch.max(result,1)\n",
    "    output_train.append(int(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SA_BERT'] = output_train\n",
    "train['SA_BERT'] -= 1\n",
    "# train.to_csv('training_set.csv', encoding='utf_8_sig', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results of the test set by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = []\n",
    "\n",
    "for i in test_x:\n",
    "    result = bertclassfication([i])\n",
    "    _, predict = torch.max(result,1)\n",
    "    output_test.append(int(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SA_BERT'] = output_test\n",
    "test['SA_BERT'] -= 1\n",
    "# test.to_csv('sentence_level_test.csv', encoding='utf_8_sig', index=None)\n",
    "# test.to_csv('article_level_test.csv', encoding='utf_8_sig', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa71afc78195d4f1c87fe6146e186b4c94fc1e8338c1dbe982e54a173891c58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
